{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfad596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regular EDA and plotting libraries\n",
    "import numpy as np # np is short for numpy\n",
    "import pandas as pd # pandas is so commonly used, it's shortened to pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # seaborn gets shortened to sns\n",
    "\n",
    "# We want our plots to appear in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "## Model evaluators\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# data preparation\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.utils import resample\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81434e05",
   "metadata": {},
   "source": [
    "## Data, Evaluation, Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17afe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the data ready - our problem is classification cos we want to classify whether wine quality is low or high.\n",
    "wine_quality = pd.read_csv(\"winequality-white.csv\")\n",
    "wine_quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f06e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aacc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality = wine_quality.rename(columns={\"fixed acidity\": \"fixed_acidity\", \"volatile acidity\": \"vol_acidity\",\"citric acid\": \"citr_acid\", \n",
    "                             \"residual sugar\": \"residual_sugar\", \"free sulfur dioxide\": \"free_sulfur_dioxide\",\n",
    "                             \"total sulfur dioxide\": \"total_sulfur_dioxide\"})\n",
    "wine_quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64014c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see label (quality) distribution in our dataframe\n",
    "wine_quality.quality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [(wine_quality['quality'] <= 5), \n",
    "              (wine_quality['quality'] >= 6)]\n",
    "\n",
    "# create a list of the values we want to assign for each condition where '0' is low, '1' is high\n",
    "values = [0, 1]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "wine_quality['label'] = np.select(conditions, values)\n",
    "\n",
    "# display updated DataFrame\n",
    "wine_quality[300:310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d30e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_quality.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebe318",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality = wine_quality.drop([\"quality\"], axis=1)\n",
    "wine_quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbe1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the value counts with a bar graph\n",
    "wine_quality.label.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35022fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_quality.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d65153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create another figure\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Start with low quality wines\n",
    "plt.scatter(wine_quality.alcohol[wine_quality.label==0], \n",
    "            wine_quality.residual_sugar\t[wine_quality.label==0],\n",
    "            c=\"magenta\") \n",
    "\n",
    "# High Quality Wines, we want them on the same plot, so we call plt again\n",
    "plt.scatter(wine_quality.alcohol[wine_quality.label==1], \n",
    "            wine_quality.residual_sugar\t[wine_quality.label==1], \n",
    "            c=\"lightblue\") # define it as a scatter figure\n",
    "\n",
    "# Add some helpful info\n",
    "plt.title(\"Wine Quality as a function of Alcohol and Residual Sugar\")\n",
    "plt.xlabel(\"Alcohol\")\n",
    "plt.legend([\"Low\", \"High\"])\n",
    "plt.ylabel(\"Residual Sugar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a837c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histograms are a great way to check the distribution of a variable\n",
    "wine_quality.alcohol.plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86384a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up random seed and create the X and y (train and test datasets)\n",
    "np.random.seed(42)\n",
    "X = wine_quality.drop([\"label\"], axis=1)\n",
    "y = wine_quality[\"label\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001401af",
   "metadata": {},
   "source": [
    "### Correlation between independent variables\n",
    "\n",
    "Finally, we'll compare all of the independent variables in one hit.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because this may give an idea of which independent variables may or may not have an impact on our target variable.\n",
    "\n",
    "We can do this using `df.corr()` which will create a [**correlation matrix**](https://www.statisticshowto.datasciencecentral.com/correlation-matrix/) for us, in other words, a big table of numbers telling us how related each variable is the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correlation between our independent variables\n",
    "corr_matrix = wine_quality.corr()\n",
    "corr_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2995914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make it look a little prettier\n",
    "corr_matrix = wine_quality.corr()\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True, \n",
    "            linewidths=0.5, \n",
    "            fmt= \".2f\", \n",
    "            cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bacd9",
   "metadata": {},
   "source": [
    "Much better. A higher positive value means a potential positive correlation (increase) and a higher negative value means a potential negative correlation (decrease)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5435c",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "We've explored the data, now we'll try to use machine learning to predict our target variable based on the 11 independent variables.\n",
    "\n",
    "And remember our evaluation metric?\n",
    "\n",
    "> If we can reach 80% accuracy at predicting whether or not a bottle of wine is high quality, we'll adopt this project.\n",
    "\n",
    "That's what we'll be aiming for.\n",
    "\n",
    "But before we build a model, we have to get our dataset ready.\n",
    "\n",
    "Let's look at it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e8dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up random seed\n",
    "np.random.seed(42)\n",
    "X = wine_quality.drop([\"label\"], axis=1)\n",
    "y = wine_quality[\"label\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45113b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalance of labels\n",
    "wine_quality.label.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);\n",
    "plt.title('Label Class Imbalance')\n",
    "plt.xlabel('Label values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59581b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f13dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ad32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60dec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b40b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "dt_scaler = scaler.fit(X_train)\n",
    "sc_X_train = dt_scaler.transform(X_train)\n",
    "sc_X_test = dt_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1 = make_imbalance(sc_X_train, y_train, \n",
    "                                  sampling_strategy={0: 1300, 1: 1300},random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7662be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fcb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_1.value_counts().plot(kind='bar', color='slateblue')\n",
    "plt.title('label balance (Undersample)')\n",
    "plt.xlabel('label values')\n",
    "plt.ylabel('Freqency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_2, y_train_2 = ros.fit_resample(sc_X_train, y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e26d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_2.value_counts().plot(kind='bar', color='orange')\n",
    "plt.title('Label Balance (Basic Upsample)')\n",
    "plt.xlabel('Label values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb68fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e50598",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state = 14)\n",
    "X_train_3, y_train_3 = smote.fit_resample(sc_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526be745",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e741dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3.value_counts().plot(kind='bar', color=[\"salmon\", \"lightblue\"])\n",
    "plt.title('Label Balance (SMOTE)')\n",
    "plt.xlabel('Label values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d11f0",
   "metadata": {},
   "source": [
    "### Model choices\n",
    "\n",
    "Now we've got our data prepared, we can start to fit models. We'll be using the following and comparing their results.\n",
    "\n",
    "1. Logistic Regression - [`LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "2. K-Nearest Neighbors - [`KNeighboursClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "3. LinearSVC -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary and fitting with Undersampling Balanced dataset\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(),\n",
    "          \"Linear SVC\": LinearSVC(max_iter=10000)}\n",
    "\n",
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train_1, sc_X_test, y_train_1, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models using Undersample Balanced Data.\n",
    "    models : a dictionary of different Scikit-Learn machine learning models\n",
    "    X_train : training data\n",
    "    X_test : testing data\n",
    "    y_train : labels associated with training data\n",
    "    y_test : labels associated with test data\n",
    "    \"\"\"\n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores_1 = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train_1, y_train_1)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores_1[name] = model.score(sc_X_test, y_test)\n",
    "    return model_scores_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed1725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_scores_1 = fit_and_score(models=models,\n",
    "                             X_train_1=X_train_1,\n",
    "                             sc_X_test=sc_X_test,\n",
    "                             y_train_1=y_train_1,\n",
    "                             y_test=y_test\n",
    "                             )\n",
    "model_scores_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary and fitting with Oversampling Balanced dataset\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(),\n",
    "          \"Linear SVC\": LinearSVC(max_iter=10000)}\n",
    "\n",
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train_2, sc_X_test, y_train_2, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models using Upsample Balanced Data.\n",
    "    models : a dictionary of different Scikit-Learn machine learning models\n",
    "    X_train : training data\n",
    "    X_test : testing data\n",
    "    y_train : labels associated with training data\n",
    "    y_test : labels associated with test data\n",
    "    \"\"\"\n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores_2 = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train_2, y_train_2)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores_2[name] = model.score(sc_X_test, y_test)\n",
    "    return model_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2def5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_2 = fit_and_score(models=models,\n",
    "                             X_train_2=X_train_2,\n",
    "                             sc_X_test=sc_X_test,\n",
    "                             y_train_2=y_train_2,\n",
    "                             y_test=y_test\n",
    "                             )\n",
    "model_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary and fitting with SMOTE Balanced dataset\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(),\n",
    "          \"Linear SVC\": LinearSVC(max_iter=10000)}\n",
    "\n",
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train_3, sc_X_test, y_train_3, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models using SMOTE balanced data.\n",
    "    models : a dictionary of different Scikit-Learn machine learning models\n",
    "    X_train : training data\n",
    "    X_test : testing data\n",
    "    y_train : labels associated with training data\n",
    "    y_test : labels associated with test data\n",
    "    \"\"\"\n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores_3 = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train_3, y_train_3)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores_3[name] = model.score(sc_X_test, y_test)\n",
    "    return model_scores_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a058321",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_3 = fit_and_score(models=models,\n",
    "                             X_train_3=X_train_3,\n",
    "                             sc_X_test=sc_X_test,\n",
    "                             y_train_3=y_train_3,\n",
    "                             y_test=y_test\n",
    "                             )\n",
    "model_scores_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Instantiate LinearSVC with all 3 balancing methods\n",
    "lsvc_model_1 = LinearSVC(max_iter=10000)\n",
    "lsvc_model_1.fit(X_train_1, y_train_1)\n",
    "lsvc_model_2 = LinearSVC(max_iter=10000)\n",
    "lsvc_model_2.fit(X_train_2, y_train_2)\n",
    "lsvc_model_3 = LinearSVC(max_iter=10000)\n",
    "lsvc_model_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "# Evaluate the LinearSVC\n",
    "print(f\"LinearSVC Model Score with Undersampling Balancing method: {lsvc_model_1.score(sc_X_test, y_test)*100:.2f}%\")\n",
    "print(f\"LinearSVC Model Score with Oversampling Balancing method: {lsvc_model_2.score(sc_X_test, y_test)*100:.2f}%\")\n",
    "print(f\"LinearSVC Model Score with SMOTE Balancing method: {lsvc_model_3.score(sc_X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Instantiate Logistic Regression with all 3 balancing methods\n",
    "logreg_model_1 = LogisticRegression()\n",
    "logreg_model_1.fit(X_train_1, y_train_1)\n",
    "logreg_model_2 = LogisticRegression()\n",
    "logreg_model_2.fit(X_train_2, y_train_2)\n",
    "logreg_model_3 = LogisticRegression()\n",
    "logreg_model_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "# Evaluate the LinearSVC\n",
    "print(f\"Logistic Regression Model Score with Undersampling Balancing method: {logreg_model_1.score(sc_X_test, y_test)*100:.2f}%\")\n",
    "print(f\"Logistic Regression Model Score with Oversampling Balancing method: {logreg_model_2.score(sc_X_test, y_test)*100:.2f}%\")\n",
    "print(f\"Logistic Regression Model Score with SMOTE Balancing method: {logreg_model_3.score(sc_X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc9ce6",
   "metadata": {},
   "source": [
    "Beautiful! Since our models are fitting, let's compare them visually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00923e17",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Since we've saved our models scores to a dictionary, we can plot them by first converting them to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7168f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Undersample Balanced Data\n",
    "model_compare_1 = pd.DataFrame(model_scores_1, index=['Accuracy'])\n",
    "model_compare_1.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c657da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upsample Balanced Data\n",
    "model_compare_2 = pd.DataFrame(model_scores_2, index=['Accuracy'])\n",
    "model_compare_2.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote Balanced Data\n",
    "model_compare_3 = pd.DataFrame(model_scores_3, index=['Accuracy'])\n",
    "model_compare_3.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48697606",
   "metadata": {},
   "source": [
    "Beautiful! We can't really see it from the graph but looking at the dictionary, the KNN model performs best.\n",
    "\n",
    "However, we can optimize the performance by each model by looking at some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88566086",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* **Hyperparameter tuning** - Each model you use has a series of dials you can turn to dictate how they perform. Changing these values may increase or decrease model performance.\n",
    "* **Feature importance** - If there are a large amount of features we're using to make predictions, do some have more importance than others? \n",
    "* [**Confusion matrix**](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) - Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line).\n",
    "* [**Cross-validation**](https://scikit-learn.org/stable/modules/cross_validation.html) - Splits your dataset into multiple parts and train and tests your model on each part and evaluates performance as an average. \n",
    "* [**Precision**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) - Proportion of true positives over total number of samples. Higher precision leads to less false positives.\n",
    "* [**Recall**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) - Proportion of true positives over total number of true positives and false negatives. Higher recall leads to less false negatives.\n",
    "* [**F1 score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) - Combines precision and recall into one metric. 1 is best, 0 is worst.\n",
    "* [**Classification report**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) - Sklearn has a built-in function called `classification_report()` which returns some of the main classification metrics such as precision, recall and f1-score.\n",
    "* [**ROC Curve**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_score.html) - [Receiver Operating Characterisitc](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a plot of true positive rate versus false positive rate.\n",
    "* [**Area Under Curve (AUC)**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) - The area underneath the ROC curve. A perfect model achieves a score of 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36e2f3",
   "metadata": {},
   "source": [
    "### Tune KNeighborsClassifier (K-Nearest Neighbors or KNN) by hand\n",
    "\n",
    "There's one main hyperparameter we can tune for the K-Nearest Neighbors (KNN) algorithm, and that is number of neighbours. The default is 5 (`n_neigbors=5`).\n",
    "\n",
    "We try a few different values of `n_neighbors`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14e85b",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of train scores\n",
    "train_scores = []\n",
    "\n",
    "# Create a list of test scores\n",
    "test_scores = []\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1, 21) # 1 to 20\n",
    "\n",
    "# Setup algorithm\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different neighbors values\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors = i) # set neighbors value\n",
    "    \n",
    "    # Fit the algorithm\n",
    "    knn.fit(X_train_1, y_train_1)\n",
    "    \n",
    "    # Update the training scores\n",
    "    train_scores.append(knn.score(X_train_1, y_train_1))\n",
    "    \n",
    "    # Update the test scores\n",
    "    test_scores.append(knn.score(sc_X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d05576d",
   "metadata": {},
   "source": [
    "Let's look at KNN's train scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd536f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdad426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(neighbors, train_scores, label=\"Train score 1\")\n",
    "plt.plot(neighbors, test_scores, label=\"Test score 1\")\n",
    "plt.xticks(np.arange(1, 21, 1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of train scores\n",
    "train_scores_2 = []\n",
    "\n",
    "# Create a list of test scores\n",
    "test_scores_2 = []\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1, 21) # 1 to 20\n",
    "\n",
    "# Setup algorithm\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different neighbors values\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors = i) # set neighbors value\n",
    "    \n",
    "    # Fit the algorithm\n",
    "    knn.fit(X_train_2, y_train_2)\n",
    "    \n",
    "    # Update the training scores\n",
    "    train_scores_2.append(knn.score(X_train_2, y_train_2))\n",
    "    \n",
    "    # Update the test scores\n",
    "    test_scores_2.append(knn.score(sc_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccda8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbors, train_scores_2, label=\"Train score 2\")\n",
    "plt.plot(neighbors, test_scores_2, label=\"Test score 2\")\n",
    "plt.xticks(np.arange(1, 21, 1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores_2)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bccbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of train scores\n",
    "train_scores_3 = []\n",
    "\n",
    "# Create a list of test scores\n",
    "test_scores_3 = []\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1, 21) # 1 to 20\n",
    "\n",
    "# Setup algorithm\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different neighbors values\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors = i) # set neighbors value\n",
    "    \n",
    "    # Fit the algorithm\n",
    "    knn.fit(X_train_3, y_train_3)\n",
    "    \n",
    "    # Update the training scores\n",
    "    train_scores_3.append(knn.score(X_train_3, y_train_3))\n",
    "    \n",
    "    # Update the test scores\n",
    "    test_scores_3.append(knn.score(sc_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a04285",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbors, train_scores_3, label=\"Train score 3\")\n",
    "plt.plot(neighbors, test_scores_3, label=\"Test score 3\")\n",
    "plt.xticks(np.arange(1, 21, 1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores_3)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638730b",
   "metadata": {},
   "source": [
    "Looking at the graph, `n_neighbors = 1` seems best.\n",
    "\n",
    "From the above, the `KNN`'s model performance is better than others but we can optimize the others two before drawing our conclusion.\n",
    "\n",
    "Instead of us having to manually try different hyperparameters by hand, `RandomizedSearchCV` tries a number of different combinations, evaluates them and saves the best.\n",
    "\n",
    "### Tuning models with with [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "\n",
    "We create a hyperparameter grid (a dictionary of different hyperparameters) for each and then test them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LogisticRegression hyperparameters\n",
    "log_reg_grid_1 = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "log_reg_grid_2 = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "log_reg_grid_3 = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Different LinearSVC hyperparameters\n",
    "log_lin_svc_1 = {\"C\": np.logspace(-4, 4, 20),\n",
    "              \"intercept_scaling\": np.logspace(-4, 4, 20)}\n",
    "log_lin_svc_2 = {\"C\": np.logspace(-4, 4, 20),\n",
    "              \"intercept_scaling\": np.logspace(-4, 4, 20)}\n",
    "log_lin_svc_3 = {\"C\": np.logspace(-4, 4, 20),\n",
    "              \"intercept_scaling\": np.logspace(-4, 4, 20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43524a3a",
   "metadata": {},
   "source": [
    "Now let's use `RandomizedSearchCV` to try and tune our `LogisticRegression` model.\n",
    "\n",
    "We'll pass it the different hyperparameters from `log_reg_grid` as well as set `n_iter = 20`. This means, `RandomizedSearchCV` will try 20 different combinations of hyperparameters from `log_reg_grid` and save the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952140f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for LogisticRegression\n",
    "rs_log_reg_1 = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid_1,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "rs_log_reg_2 = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid_2,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "rs_log_reg_3 = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid_3,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_log_reg_1.fit(X_train_1, y_train_1);\n",
    "rs_log_reg_2.fit(X_train_2, y_train_2);\n",
    "rs_log_reg_3.fit(X_train_3, y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs_log_reg_1.best_params_)\n",
    "print(rs_log_reg_2.best_params_)\n",
    "print(rs_log_reg_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs_log_reg_1.score(sc_X_test, y_test)) # tuning hyperparameters C and solver gives us a lower degree of accuracy \n",
    "print(rs_log_reg_2.score(sc_X_test, y_test))                           \n",
    "print(rs_log_reg_3.score(sc_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for LinearSVC\n",
    "rs_lin_svc_1 = RandomizedSearchCV(LinearSVC(),\n",
    "                                param_distributions=log_lin_svc_1,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "rs_lin_svc_2 = RandomizedSearchCV(LinearSVC(),\n",
    "                                param_distributions=log_lin_svc_2,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "rs_lin_svc_3 = RandomizedSearchCV(LinearSVC(),\n",
    "                                param_distributions=log_lin_svc_3,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_lin_svc_1.fit(X_train_1, y_train_1);\n",
    "rs_lin_svc_2.fit(X_train_2, y_train_2);\n",
    "rs_lin_svc_3.fit(X_train_3, y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs_lin_svc_1.best_params_)\n",
    "print(rs_lin_svc_2.best_params_)\n",
    "print(rs_lin_svc_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db091aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs_lin_svc_1.score(sc_X_test, y_test)) # tuning hyperparameters C and intercept_scaling gives \n",
    "print(rs_lin_svc_2.score(sc_X_test, y_test)) # us a lower degree of accuracy\n",
    "print(rs_lin_svc_3.score(sc_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken to train the lInearSVC model.\n",
    "%timeit rs_lin_svc_1.fit(X_train_1, y_train_1);\n",
    "%timeit rs_lin_svc_2.fit(X_train_2, y_train_2);\n",
    "%timeit rs_lin_svc_3.fit(X_train_3, y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675df42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken to predict using the linearSVC model.\n",
    "%timeit lsvc_model_1.predict(sc_X_test)\n",
    "%timeit lsvc_model_2.predict(sc_X_test)\n",
    "%timeit lsvc_model_3.predict(sc_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec088e",
   "metadata": {},
   "source": [
    "Excellent! Tuning the hyperparameters for each model saw a slight reduction in performance boost in both the `LinearSVC` and `LogisticRegression`.\n",
    "\n",
    "This is akin to tuning the settings on your oven and getting it to cook your favourite dish just right.\n",
    "\n",
    "We give `LogisticRegression` another try to see if we can pull out something better using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "### Tuning a model with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "The difference between `RandomizedSearchCV` and `GridSearchCV` is where `RandomizedSearchCV` searches over a grid of hyperparameters performing `n_iter` combinations, `GridSearchCV` will test every single possible combination.\n",
    "\n",
    "In short:\n",
    "* `RandomizedSearchCV` - tries `n_iter` combinations of hyperparameters and saves the best.\n",
    "* `GridSearchCV` - tries every single combination of hyperparameters and saves the best.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedea664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LogisticRegression hyperparameters\n",
    "gs_log_reg_grid_1 = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "gs_log_reg_grid_2 = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "gs_log_reg_grid_3 = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_log_reg_1 = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid_1,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "gs_log_reg_2 = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid_2,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "gs_log_reg_3 = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid_3,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_reg_1.fit(X_train_1, y_train_1);\n",
    "gs_log_reg_2.fit(X_train_2, y_train_2);\n",
    "gs_log_reg_3.fit(X_train_3, y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be66962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best parameters\n",
    "print(gs_log_reg_1.best_params_)\n",
    "print(gs_log_reg_2.best_params_)\n",
    "print(gs_log_reg_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(gs_log_reg_1.score(sc_X_test, y_test))\n",
    "print(gs_log_reg_2.score(sc_X_test, y_test))\n",
    "print(gs_log_reg_3.score(sc_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken to train the logistic Regression model.\n",
    "%timeit gs_log_reg_1.fit(X_train_1, y_train_1);\n",
    "%timeit gs_log_reg_2.fit(X_train_2, y_train_2);\n",
    "%timeit gs_log_reg_3.fit(X_train_3, y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken to predict using the logistic Regression model.\n",
    "%timeit logreg_model_1.predict(sc_X_test)\n",
    "%timeit logreg_model_2.predict(sc_X_test)\n",
    "%timeit logreg_model_3.predict(sc_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model_1.fit(X_train_1, y_train_1);\n",
    "knn_model_2 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model_2.fit(X_train_2, y_train_2);\n",
    "knn_model_3 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model_3.fit(X_train_3, y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken to train the KNN model.\n",
    "%timeit knn_model_1.fit(X_train_1, y_train_1);\n",
    "%timeit knn_model_2.fit(X_train_2, y_train_2);\n",
    "%timeit knn_model_3.fit(X_train_3, y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken to predict the KNN model.\n",
    "%timeit knn_model_1.predict(sc_X_test)\n",
    "%timeit knn_model_2.predict(sc_X_test)\n",
    "%timeit knn_model_3.predict(sc_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fc374",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds_1 = knn_model_1.predict(sc_X_test)\n",
    "knn_preds_2 = knn_model_2.predict(sc_X_test)\n",
    "knn_preds_3 = knn_model_3.predict(sc_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528020fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds_2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1290d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f393f53",
   "metadata": {},
   "source": [
    "## Evaluating a classification model, beyond accuracy\n",
    "\n",
    "Now we've got a tuned model, let's get some of the metrics we discussed before.\n",
    "\n",
    "We want:\n",
    "* ROC curve and AUC score - [`plot_roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html#sklearn.metrics.plot_roc_curve)\n",
    "* Confusion matrix - [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "* Classification report - [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "* Precision - [`precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "* Recall - [`recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "* F1-score - [`f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "\n",
    "To access them, we'll have to use our model to make predictions on the test set. You can make predictions by calling `predict()` on a trained model and passing it the data you'd like to predict on.\n",
    "\n",
    "We'll make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data for Logistic Regression\n",
    "lr_preds_1 = logreg_model_1.predict(sc_X_test)\n",
    "lr_preds_2 = logreg_model_2.predict(sc_X_test)\n",
    "lr_preds_3 = logreg_model_3.predict(sc_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4dc5ea",
   "metadata": {},
   "source": [
    "Let's see them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1f82b",
   "metadata": {},
   "source": [
    "They look like our original test data labels, except different where the model has predicred wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data for LinearSVC\n",
    "lsvc_preds_1 = lsvc_model_1.predict(sc_X_test)\n",
    "lsvc_preds_2 = lsvc_model_2.predict(sc_X_test)\n",
    "lsvc_preds_3 = lsvc_model_3.predict(sc_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bffdd2",
   "metadata": {},
   "source": [
    "Since we've got our prediction values we can find the metrics we want.\n",
    "\n",
    "Let's start with the ROC curve and AUC scores.\n",
    "\n",
    "### ROC Curve and AUC Scores\n",
    "\n",
    "It's a way of understanding how our model is performing by comparing the true positive rate to the false positive rate.\n",
    "\n",
    "In our case...\n",
    "\n",
    "A false positive in this case occurs when the wine quality is predicted as high but is actually low. A false negative, on the other hand, occurs when the wine quality is predicted as low when they are actually high quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Instantiate LinearSVC with all 3 balancing methods\n",
    "logreg_model_1 = LogisticRegression()\n",
    "logreg_model_1.fit(X_train_1, y_train_1)\n",
    "logreg_model_2 = LogisticRegression()\n",
    "logreg_model_2.fit(X_train_2, y_train_2)\n",
    "logreg_model_3 = LogisticRegression()\n",
    "logreg_model_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "# Evaluate the LinearSVC\n",
    "print(f\"Logistic Regression Model Score with Undersampling Balancing method: {logreg_model_1.score(sc_X_test, y_test)*100:.2f}%\")\n",
    "print(f\"Logistic Regression Model Score with Oversampling Balancing method: {logreg_model_2.score(sc_X_test, y_test)*100:.2f}%\")\n",
    "print(f\"Logistic Regression Model Score with SMOTE Balancing method: {logreg_model_3.score(sc_X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds_1 = logreg_model_1.predict(sc_X_test)\n",
    "lr_preds_2 = logreg_model_2.predict(sc_X_test)\n",
    "lr_preds_3 = logreg_model_3.predict(sc_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88416fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set up plotting area\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "#plt.figure(0).clf()\n",
    "\n",
    "#fit logistic regression model and plot ROC curve\n",
    "logreg_model_1 = LogisticRegression()\n",
    "logreg_model_1.fit(X_train_1, y_train_1)\n",
    "lr_preds_1 = logreg_model_1.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, lr_preds_1)\n",
    "auc = round(metrics.roc_auc_score(y_test, lr_preds_1), 2)\n",
    "plt.plot(fpr,tpr,label=\"UnderSampling, AUC=\"+str(auc))\n",
    "\n",
    "logreg_model_2 = LogisticRegression()\n",
    "logreg_model_2.fit(X_train_2, y_train_2)\n",
    "lr_preds_2 = logreg_model_2.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, lr_preds_2)\n",
    "auc = round(metrics.roc_auc_score(y_test, lr_preds_2), 2)\n",
    "plt.plot(fpr,tpr,label=\"OverSampling, AUC=\"+str(auc))\n",
    "\n",
    "logreg_model_3 = LogisticRegression()\n",
    "logreg_model_3.fit(X_train_3, y_train_3)\n",
    "lr_preds_3 = logreg_model_2.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, lr_preds_3)\n",
    "auc = round(metrics.roc_auc_score(y_test, lr_preds_3), 2)\n",
    "plt.plot(fpr,tpr, label=\"SMOTE Sampling, AUC=\"+str(auc))\n",
    "#plt.plot\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "#add legend\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit KNN model and plot ROC curve\n",
    "knn_model_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model_1.fit(X_train_1, y_train_1)\n",
    "knn_preds_1 = knn_model_1.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, knn_preds_1)\n",
    "auc = round(metrics.roc_auc_score(y_test, knn_preds_1), 2)\n",
    "plt.plot(fpr,tpr,label=\"UnderSampling, AUC=\"+str(auc))\n",
    "\n",
    "knn_model_2 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model_2.fit(X_train_2, y_train_2)\n",
    "knn_preds_2 = knn_model_2.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, knn_preds_2)\n",
    "auc = round(metrics.roc_auc_score(y_test, knn_preds_2), 2)\n",
    "plt.plot(fpr,tpr,label=\"OverSampling, AUC=\"+str(auc))\n",
    "\n",
    "knn_model_3 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model_3.fit(X_train_3, y_train_3)\n",
    "knn_preds_3 = knn_model_3.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, knn_preds_3)\n",
    "auc = round(metrics.roc_auc_score(y_test, knn_preds_3), 2)\n",
    "plt.plot(fpr,tpr,label=\"SMOTE Sampling, AUC=\"+str(auc))\n",
    "#plt.plot\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "#plt.xlim([-0.01, 1.01])\n",
    "#plt.ylim([0,1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('K-Neighbours ROC Curve')\n",
    "#add legend\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d34418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit LinearSVC model and plot ROC curve\n",
    "lsvc_model_1 = LinearSVC(max_iter=10000)\n",
    "lsvc_model_1.fit(X_train_1, y_train_1)\n",
    "lsvc_preds_1 = lsvc_model_1.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, lsvc_preds_1)\n",
    "auc = round(metrics.roc_auc_score(y_test, lsvc_preds_1), 2)\n",
    "plt.plot(fpr,tpr,label=\"UnderSampling, AUC=\"+str(auc))\n",
    "\n",
    "lsvc_model_2 = LinearSVC(max_iter=10000)\n",
    "lsvc_model_2.fit(X_train_2, y_train_2)\n",
    "lsvc_preds_2 = lsvc_model_2.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, lsvc_preds_2)\n",
    "auc = round(metrics.roc_auc_score(y_test, lsvc_preds_2), 2)\n",
    "plt.plot(fpr,tpr,label=\"OverSampling, AUC=\"+str(auc))\n",
    "\n",
    "lsvc_model_3 = LinearSVC(max_iter=10000)\n",
    "lsvc_model_3.fit(X_train_3, y_train_3)\n",
    "lsvc_preds_3 = lsvc_model_3.predict(sc_X_test) #[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, lsvc_preds_3)\n",
    "auc = round(metrics.roc_auc_score(y_test, lsvc_preds_3), 2)\n",
    "plt.plot(fpr,tpr,label=\"SMOTE Sampling, AUC=\"+str(auc))\n",
    "#plt.plot\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "#plt.xlim([-0.01, 1.01])\n",
    "#plt.ylim([0,1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('LinearSVC ROC Curve')\n",
    "#add legend\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f2913",
   "metadata": {},
   "source": [
    "Our model does far better than guessing which would be a line going from the bottom left corner to the top right corner, AUC = 0.72 But a perfect model would achieve an AUC score of 1.0, so there's still room for improvement.\n",
    "\n",
    "\n",
    "### Confusion matrix \n",
    "\n",
    "A confusion matrix is a visual way to show where your model made the right predictions and where it made the wrong predictions (or in other words, got confused).\n",
    "\n",
    "Scikit-Learn allows us to create a confusion matrix using [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and passing it the true labels and predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665e0a9",
   "metadata": {},
   "source": [
    "Since we are presenting a paper we want to make it visual.\n",
    "\n",
    "Let's create a function which uses Seaborn's [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a11c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, lr_preds_1):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, lr_preds_1),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis \n",
    "    plt.title(\"LogisticRegression Undersampling\\n Confusion Matrix\")\n",
    "plot_conf_mat(y_test, lr_preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, lr_preds_2):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, lr_preds_2),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis \n",
    "    plt.title(\"LogisticRegression Oversampling\\n Confusion Matrix\")\n",
    "plot_conf_mat(y_test, lr_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(font_scale=1) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, lr_preds_3):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, lr_preds_3),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis \n",
    "    plt.title(\"LogisticRegression SMOTE sampling\\n Confusion Matrix\")\n",
    "plot_conf_mat(y_test, lr_preds_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43419a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(font_scale=1) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, lsvc_preds_1):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, lsvc_preds_1),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis \n",
    "    plt.title(\"LinearSVC Undersampling\\n Confusion Matrix\")\n",
    "plot_conf_mat(y_test, lsvc_preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7daf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(font_scale=1) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, lsvc_preds_2):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, lsvc_preds_2),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis \n",
    "    plt.title(\"LinearSVC Oversampling\\n Confusion Matrix\")\n",
    "plot_conf_mat(y_test, lsvc_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a507511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(font_scale=1) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, lsvc_preds_3):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, lsvc_preds_3),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis \n",
    "    plt.title(\"LinearSVC SMOTE sampling\\n Confusion Matrix\")\n",
    "plot_conf_mat(y_test, lsvc_preds_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3eacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(y_test, knn_preds_1):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, knn_preds_1),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis\n",
    "    plt.title(\"KNN Undersampling\\n Confusion Matrix\")\n",
    "    sns.set(font_scale=1) \n",
    "    \n",
    "plot_conf_mat(y_test, knn_preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bacd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(y_test, knn_preds_2):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, knn_preds_2),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis\n",
    "    plt.title(\"KNN Oversampling\\n Confusion Matrix\")\n",
    "    \n",
    "plot_conf_mat(y_test, knn_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(y_test, knn_preds_3):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, knn_preds_3),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel(\"Predicted Quality\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True Quality\") # true labels go on the y-axis \n",
    "    plt.title(\"KNN SMOTE\\n Confusion Matrix\") # predictions go on the x-axis\n",
    "    \n",
    "plot_conf_mat(y_test, knn_preds_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce2f4f",
   "metadata": {},
   "source": [
    "Beautiful! That looks much better. \n",
    "\n",
    "We can see the model gets confused (predicts the wrong label) relatively the same across both classes. In essence, there are several occasaions where the model predicted 0 when it should've been 1 (false negative) and occasions where the model predicted 1 instead of 0 (false positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958d3a4",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "\n",
    "We can make a classification report using [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) and passing it the true labels as well as our models predicted labels. \n",
    "\n",
    "A classification report will also give us information of the precision and recall of our model for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880337cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "print(classification_report(y_test, knn_preds_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, knn_preds_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f6c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, knn_preds_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85340848",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lr_preds_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lr_preds_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lr_preds_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lsvc_preds_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lsvc_preds_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lsvc_preds_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658308b",
   "metadata": {},
   "source": [
    "What's going on here?\n",
    "\n",
    "Let's get a refresh.\n",
    "\n",
    "* **Precision** - Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n",
    "* **Recall** - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n",
    "* **F1 score** - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n",
    "* **Support** - The number of samples each metric was calculated on.\n",
    "* **Accuracy** - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0.\n",
    "* **Macro avg** - Short for macro average, the average precision, recall and F1 score between classes. Macro avg doesn’t class imbalance into effort, so if you do have class imbalances, pay attention to this metric.\n",
    "* **Weighted avg** - Short for weighted average, the weighted average precision, recall and F1 score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favour the majority class (e.g. will give a high value when one class out performs another due to having more samples).\n",
    "\n",
    "Ok, now we've got a few deeper insights on our model. But these were all calculated using a single training and test set.\n",
    "\n",
    "What we'll do to make them more solid is calculate them using cross-validation.\n",
    "\n",
    "How?\n",
    "\n",
    "We'll take the best model along with the best hyperparameters and use [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) along with various `scoring` parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42690236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check best hyperparameters\n",
    "# gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e15e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate best model with best hyperparameters (found with GridSearchCV)\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58ec24",
   "metadata": {},
   "source": [
    "Now we've got an instantiated classifier, let's find some cross-validated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated accuracy score\n",
    "cv_acc_1 = cross_val_score(clf,\n",
    "                         X_train_1,\n",
    "                         y_train_1,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_2 = cross_val_score(clf,\n",
    "                         X_train_2,\n",
    "                         y_train_2,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_3 = cross_val_score(clf,\n",
    "                         X_train_3,\n",
    "                         y_train_3,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "print('Cross Validtion Accuracy Score with Undersampling', cv_acc_1)\n",
    "print('Cross Validtion Accuracy Score with Oversampling', cv_acc_2)\n",
    "print('Cross Validtion Accuracy Score with SMOTE sampling', cv_acc_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f16190",
   "metadata": {},
   "source": [
    "Since there are 5 metrics here, we'll take the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "dt_scaler = scaler.fit(X)\n",
    "X_scale = dt_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b24a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wine_quality[\"label\"]\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing it. Undersampling\n",
    "X_scale_1, y1 = make_imbalance(X_scale, y, \n",
    "                                  sampling_strategy={0: 1300, 1: 1300},random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing it Oversampling\n",
    "X_scale_2, y2 = ros.fit_resample(X_scale, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing it SMOTE - Oversampling\n",
    "X_scale_3, y3 = smote.fit_resample(X_scale, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3595aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate best model with best hyperparameters (found with GridSearchCV)\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_lr1 = cross_val_score(clf,\n",
    "                         X_scale_1,\n",
    "                         y1,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_lr2 = cross_val_score(clf,\n",
    "                         X_scale_2,\n",
    "                         y2,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_lr3 = cross_val_score(clf,\n",
    "                         X_scale_3,\n",
    "                         y3,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "print('Cross Validtion Accuracy Score with Undersampling', cv_acc_lr1)\n",
    "print('Cross Validtion Accuracy Score with Oversampling', cv_acc_lr2)\n",
    "print('Cross Validtion Accuracy Score with SMOTE sampling', cv_acc_lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_lrm1 = np.mean(cv_acc_lr1)\n",
    "cv_acc_lrm2 = np.mean(cv_acc_lr2)\n",
    "cv_acc_lrm3 = np.mean(cv_acc_lr3)\n",
    "print(cv_acc_lrm1)\n",
    "print(cv_acc_lrm2)\n",
    "print(cv_acc_lrm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e52c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# Cross-validated accuracy score\n",
    "cv_acc_knn1 = cross_val_score(clf_knn,\n",
    "                         X_scale_1,\n",
    "                         y1,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_knn2 = cross_val_score(clf_knn,\n",
    "                         X_scale_2,\n",
    "                         y2,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_knn3 = cross_val_score(clf_knn,\n",
    "                         X_scale_3,\n",
    "                         y3,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "print('Cross Validtion Accuracy Score with Undersampling', cv_acc_knn1)\n",
    "print('Cross Validtion Accuracy Score with Oversampling', cv_acc_knn2)\n",
    "print('Cross Validtion Accuracy Score with SMOTE sampling', cv_acc_knn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3815d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_knn_m1 = np.mean(cv_acc_knn1)\n",
    "cv_acc_knn_m2 = np.mean(cv_acc_knn2)\n",
    "cv_acc_knn_m3 = np.mean(cv_acc_knn3)\n",
    "print(cv_acc_knn_m1)\n",
    "print(cv_acc_knn_m2)\n",
    "print(cv_acc_knn_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lsvc = LinearSVC(max_iter=10000)\n",
    "\n",
    "# Cross-validated accuracy score\n",
    "cv_acc_lsvc1 = cross_val_score(clf_lsvc,\n",
    "                         X_scale_1,\n",
    "                         y1,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_lsvc2 = cross_val_score(clf_lsvc,\n",
    "                         X_scale_2,\n",
    "                         y2,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc_lsvc3 = cross_val_score(clf_lsvc,\n",
    "                         X_scale_3,\n",
    "                         y3,\n",
    "                         cv=5, # 5-fold cross-validation\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "print('Cross Validtion Accuracy Score with Undersampling', cv_acc_lsvc1)\n",
    "print('Cross Validtion Accuracy Score with Oversampling', cv_acc_lsvc2)\n",
    "print('Cross Validtion Accuracy Score with SMOTE sampling', cv_acc_lsvc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53494d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_lsvc_m1 = np.mean(cv_acc_lsvc1)\n",
    "cv_acc_lsvc_m2 = np.mean(cv_acc_lsvc2)\n",
    "cv_acc_lsvc_m3 = np.mean(cv_acc_lsvc3)\n",
    "print(cv_acc_lsvc_m1)\n",
    "print(cv_acc_lsvc_m2)\n",
    "print(cv_acc_lsvc_m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27bbd4",
   "metadata": {},
   "source": [
    "\n",
    "Another model evaluation techniques is feature importance.\n",
    "\n",
    "## Feature importance\n",
    "\n",
    "Feature importance is another way of asking, \"which features contributing most to the outcomes of the model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee232215",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = dict(zip(wine_quality.columns, list(logreg_model_3.coef_[0])))\n",
    "features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f42c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "features_df = pd.DataFrame(features_dict, index=[0])\n",
    "features_df.T.plot.bar(title=\"Feature Importance\", legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7adda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
